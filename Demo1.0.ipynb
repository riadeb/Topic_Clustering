{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Package Import\n",
    "\n",
    "from __future__ import absolute_import, division, print_function, unicode_literals\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "import pandas as pd\n",
    "import requests\n",
    "import numpy as np\n",
    "import tensorflow_hub as hub\n",
    "import os\n",
    "from tensorflow.keras.layers import Conv1D\n",
    "from tensorflow.keras.layers import MaxPooling1D\n",
    "from tensorflow.keras.layers import GlobalAveragePooling1D\n",
    "from tensorflow.keras.layers import Reshape\n",
    "from tensorflow.keras.layers import Dropout\n",
    "# from tensorflow.keras.layers.merge import add\n",
    "from tensorflow.keras.layers import LSTM, Embedding, Dense, TimeDistributed, Dropout, Bidirectional, Lambda, Input, Flatten\n",
    "from tensorflow.keras import Model\n",
    "from sklearn import preprocessing\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from tensorflow.python.framework import ops\n",
    "ops.reset_default_graph()\n",
    "import tensorflow.compat.v1 as tf\n",
    "#To make tf 2.0 compatible with tf1.0 code, we disable the tf2.0 functionalities\n",
    "tf.disable_eager_execution()\n",
    "from tensorflow.keras import backend\n",
    "backend.clear_session()  # For easy reset of notebook state.\n",
    "\n",
    "from topic_clustering_models import Modeling\n",
    "from sub_intents_detection import DataPreprocessing, SubIntentsModeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chatting bot based on Deep Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b style = \"color:red\">Important!</b>\n",
    "\n",
    "**This demo could only be run with Internet Connetion. Because our dataset and part of the model is online**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dataset url https://github.com/google-research-datasets/Taskmaster/raw/master/TM-1-2019"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "__author__ = [\"Haolin Pan\", \"Riade Benbaki\"]\n",
    "__version__ = \"Ã‰cole Polytechnique, 2020/2/21\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "__data__ = \"https://github.com/google-research-datasets/Taskmaster/raw/master/TM-1-2019/self-dialogs.json\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Topic Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'tensorflow' has no attribute 'get_default_graph'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-4-4fa48e789927>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0ms\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mModeling\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0ms\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdataPreprocessing\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0ms\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodelBuildingAndTraining\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\source\\repos\\python_jupiter_programs\\Chatting_bot_based_on_Elmo\\topic_clustering_models.py\u001b[0m in \u001b[0;36mmodelBuildingAndTraining\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    133\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdataPreprocessing\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    134\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 135\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodel_lstm\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbuild_model_lstm\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    136\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexists\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"topic_clustering_model/lstm.h5\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    137\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodel_lstm\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload_weights\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"topic_clustering_model/lstm.h5\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\source\\repos\\python_jupiter_programs\\Chatting_bot_based_on_Elmo\\topic_clustering_models.py\u001b[0m in \u001b[0;36mbuild_model_lstm\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    104\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    105\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mbuild_model_lstm\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 106\u001b[1;33m         \u001b[0minput_text1\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mInput\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstring\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    107\u001b[0m         \u001b[0membeding1\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mLambda\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mELMOO\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moutput_shape\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1024\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput_text1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    108\u001b[0m         \u001b[0mreshape\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mReshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1024\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput_shape\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1024\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0membeding1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\keras\\engine\\input_layer.py\u001b[0m in \u001b[0;36mInput\u001b[1;34m(shape, batch_shape, name, dtype, sparse, tensor)\u001b[0m\n\u001b[0;32m    176\u001b[0m                              \u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    177\u001b[0m                              \u001b[0msparse\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msparse\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 178\u001b[1;33m                              input_tensor=tensor)\n\u001b[0m\u001b[0;32m    179\u001b[0m     \u001b[1;31m# Return tensor including _keras_shape and _keras_history.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    180\u001b[0m     \u001b[1;31m# Note that in this case train_output and test_output are the same pointer.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\keras\\legacy\\interfaces.py\u001b[0m in \u001b[0;36mwrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     89\u001b[0m                 warnings.warn('Update your `' + object_name + '` call to the ' +\n\u001b[0;32m     90\u001b[0m                               'Keras 2 API: ' + signature, stacklevel=2)\n\u001b[1;32m---> 91\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     92\u001b[0m         \u001b[0mwrapper\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_original_function\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     93\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\keras\\engine\\input_layer.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, input_shape, batch_size, batch_input_shape, dtype, input_tensor, sparse, name)\u001b[0m\n\u001b[0;32m     37\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     38\u001b[0m             \u001b[0mprefix\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'input'\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 39\u001b[1;33m             \u001b[0mname\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mprefix\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m'_'\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mK\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_uid\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprefix\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     40\u001b[0m         \u001b[0msuper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mInputLayer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     41\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py\u001b[0m in \u001b[0;36mget_uid\u001b[1;34m(prefix)\u001b[0m\n\u001b[0;32m     72\u001b[0m     \"\"\"\n\u001b[0;32m     73\u001b[0m     \u001b[1;32mglobal\u001b[0m \u001b[0m_GRAPH_UID_DICTS\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 74\u001b[1;33m     \u001b[0mgraph\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_default_graph\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     75\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mgraph\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32min\u001b[0m \u001b[0m_GRAPH_UID_DICTS\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     76\u001b[0m         \u001b[0m_GRAPH_UID_DICTS\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mgraph\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdefaultdict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mint\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: module 'tensorflow' has no attribute 'get_default_graph'"
     ]
    }
   ],
   "source": [
    "s = Modeling()\n",
    "s.dataPreprocessing()\n",
    "s.modelBuildingAndTraining()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, we try to detect an opening phrase of one scenario, one conversation; \n",
    "\n",
    "Normally, this would be a phrase to express a need. For example, <i>\" I want a uber to the airport \"</i>\n",
    "\n",
    "Otherwise if the phrase has no information about the scenario, for expample, <i>\\\" Ok, that's it \\\"</i> we regard this as  non-opening\n",
    "\n",
    "Then between two opening phrases, we have a more stable scenario which allow us to make detection more precise."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The precision based on a 0.2 percentage validation set is up to 93%**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test the models of topic clustering: \n",
      "================================================================\n",
      "The predicted topic of \"Where is the nearest Starbucks ? \" is : coffee\n",
      "The predicted topic of \"i need to repair my car \" is : auto\n",
      "The predicted topic of \"I need a ride from home \" is : uber\n",
      "The predicted topic of \"I want to order something to eat \" is : pizza\n",
      "The predicted topic of \"can you activate \" is : non-opening\n",
      "The predicted topic of \"I want a table in center city \" is : restaurant\n",
      "The predicted topic of \"Ok that's it! \" is : non-opening\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "print(\"Test the models of topic clustering: \")\n",
    "print(64 * \"=\")\n",
    "test_phrases = [[\"Where is the nearest Starbucks ?\"],[\"i need to repair my car\"],\\\n",
    "                [\"I need a ride from home\"],[\"I want to order something to eat\"],[\"can you activate\"],\\\n",
    "                [\"I want a table in center city\"], [\"Ok that's it!\"]]\n",
    "res = s.prediction(test_phrases)\n",
    "for i in range(len(res)):\n",
    "    print(\"The predicted topic of \\\"{} \\\" is : {}\".format(test_phrases[i][0], res[i]))\n",
    "print(64 * \"-\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Subintents Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n"
     ]
    }
   ],
   "source": [
    "dp = DataPreprocessing()\n",
    "i2phr = dp.intents_clustering()\n",
    "SIM = SubIntentsModeling(i2phr)\n",
    "if not SIM.trained():\n",
    "    SIM.training_models(\"restaurant\")\n",
    "    SIM.training_models(\"movie\")\n",
    "    SIM.training_models(\"auto\")\n",
    "    SIM.training_models(\"pizza\")\n",
    "    SIM.training_models(\"uber\")\n",
    "    SIM.training_models(\"coffee\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After detecting a opening phrase of one topic, we try to detect the intents more specific for the topic\n",
    "\n",
    "For example, in the topic of restaurant reservation, we would like to get the informations like <i>time </i>, <i>location</i> etc.\n",
    "\n",
    "Luckily, the entities which reveals this information is labeled in our data set. \n",
    "\n",
    "For example, in the phrase, <i>Thank you, 8 pm will be fine.</i>, <i>8 pm</i> is marked out as the time information\n",
    "\n",
    "In our model, we try to detect the if the phrase has some information we need, if yes, what type is it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The precision of this model is about 0.7 to 0.8**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test the models of Sub intent detection: \n",
      "================================================================\n",
      "The predicted intent of \"Where is the nearest restaurant?\" is location\n",
      "The predicted intent of \"This Evening\" is time\n",
      "The predicted intent of \"I want this\" is others\n",
      "The predicted intent of \"We have totally 4\" is num\n",
      "The predicted intent of \"My dad, mom, me and my sister\" is num\n",
      "The predicted intent of \"Do you have some recommends\" is type\n",
      "The predicted intent of \"Thank you! Bye\" is name\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "print(\"Test the models of Sub intent detection: \")\n",
    "print(64 * \"=\")\n",
    "test_phrases = np.array([[\"Where is the nearest restaurant?\"], [\"This Evening\"], [\"I want this\"],\n",
    "                  [\"We have totally 4\"], [\"My dad, mom, me and my sister\"], [\"Do you have some recommends\"],\n",
    "                  [\"Thank you! Bye\"]])\n",
    "res = SIM.prediction(\"restaurant\", test_phrases)\n",
    "for i in range(len(res)):\n",
    "    print(\"The predicted intent of \\\"{}\\\" is {}\".format(test_phrases[i][0], res[i]))\n",
    "print(64 * \"-\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__TO DO__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Information Obtaining"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Response Generation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conversation Demo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "topic allowed: 'auto repair', 'coffee ordering', 'movie booking', 'pizza ordering','restaurant reservation','uber ordering'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Demo_Conversation(object):\n",
    "    def __init__(self, model_cl, models_si):\n",
    "        self.model_cl = model_cl\n",
    "        self.models_si = models_si\n",
    "        self.classes_arr = ['auto', 'coffee', 'movie', 'non-opening', 'pizza','restaurant','uber']\n",
    "        self.opening_response = {\"auto\" : \"\\tIt seems you want to repair your vehicle\", \\\n",
    "                            \"coffee\" : \"\\tIt seems you want to order some coffee\", \\\n",
    "                            \"movie\" : \"\\tIt seems you want to book movie tickets\", \\\n",
    "                            \"pizza\" : \"\\tIt seems you want to order some pizza\",\\\n",
    "                            \"restaurant\": \"\\tIt seems you want to book a table\",\\\n",
    "                            \"uber\": \"\\tIt seems you want to take ride\"}\n",
    "\n",
    "    def inConversationDetected(self):\n",
    "        \n",
    "        print(\"Conversation Test\")\n",
    "        print(\"Still working on it, not completed\")\n",
    "        print(\"Input \\'~\\' to stop\")\n",
    "        print(64*\"=\")\n",
    "        print(\"\\tHello, what can I do for you?\")\n",
    "        print(64 * \"-\")\n",
    "        userInput = \"\"\n",
    "        current_topic = \"non-opening\";\n",
    "        userInput = input()\n",
    "        \n",
    "        while current_topic == \"non-opening\":\n",
    "            prediction = self.model_cl.predict(np.array([[userInput], [userInput]]))\n",
    "            class_pred = self.classes_arr[np.argmax(prediction[0],axis=0)]\n",
    "            current_topic = class_pred\n",
    "            \n",
    "        print(\"\\ttopic predicted: \", class_pred)\n",
    "        print(\"\\tentering the scenario \", class_pred)\n",
    "        if class_pred not in self.classes_arr:\n",
    "            print(\"Error, there is no class\")\n",
    "            return -1\n",
    "        print(self.opening_response[class_pred])\n",
    "\n",
    "        \n",
    "        while (userInput != \"~\"):  \n",
    "            subintent = self.models_si.prediction(current_topic, np.array([[userInput],[userInput]]))[0]\n",
    "            print(\"\\tIt seems that you want to say something about: \", subintent)    \n",
    "            print(64* \"-\")\n",
    "            userInput = input()\n",
    "        \n",
    "        print(\"Goodbye, thank you for your attention\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "dc = Demo_Conversation(s.model_lstm, SIM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conversation Test\n",
      "Still working on it, not completed\n",
      "Input '~' to stop\n",
      "================================================================\n",
      "\tHello, what can I do for you?\n",
      "----------------------------------------------------------------\n",
      "I want to see some newest movies\n",
      "\ttopic predicted:  movie\n",
      "\tentering the scenario  movie\n",
      "\tIt seems you want to book movie tickets\n",
      "\tIt seems that you want to say something about:  name\n",
      "----------------------------------------------------------------\n",
      "this evening\n",
      "\tIt seems that you want to say something about:  time\n",
      "----------------------------------------------------------------\n",
      "Jusr in Paris\n",
      "\tIt seems that you want to say something about:  location\n",
      "----------------------------------------------------------------\n",
      "We have 4 persons totally, my parents , me and my sister\n",
      "\tIt seems that you want to say something about:  num\n",
      "----------------------------------------------------------------\n",
      "Wonder woman 2 would be great\n",
      "\tIt seems that you want to say something about:  name\n",
      "----------------------------------------------------------------\n",
      "That's it! book it for me\n",
      "\tIt seems that you want to say something about:  name\n",
      "----------------------------------------------------------------\n",
      "~\n",
      "Goodbye, thank you for your attention\n"
     ]
    }
   ],
   "source": [
    "dc.inConversationDetected()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conversation Test\n",
      "Still working on it, not completed\n",
      "Input '~' to stop\n",
      "================================================================\n",
      "\tHello, what can I do for you?\n",
      "----------------------------------------------------------------\n",
      "I think my car is broken\n",
      "\ttopic predicted:  auto\n",
      "\tentering the scenario  auto\n",
      "\tIt seems you want to repair your vehicle\n",
      "\tIt seems that you want to say something about:  reason\n",
      "----------------------------------------------------------------\n",
      "There was a little accident\n",
      "\tIt seems that you want to say something about:  reason\n",
      "----------------------------------------------------------------\n",
      "It's a old car, it is 20 years old\n",
      "\tIt seems that you want to say something about:  name\n",
      "----------------------------------------------------------------\n",
      "It's a BMW\n",
      "\tIt seems that you want to say something about:  name\n",
      "----------------------------------------------------------------\n",
      "Could you order repair work next week\n",
      "\tIt seems that you want to say something about:  date\n",
      "----------------------------------------------------------------\n",
      "That's too kind\n",
      "\tIt seems that you want to say something about:  reason\n",
      "----------------------------------------------------------------\n",
      "~\n",
      "Goodbye, thank you for your attention\n"
     ]
    }
   ],
   "source": [
    "dc.inConversationDetected()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conversation Test\n",
      "Still working on it, not completed\n",
      "Input '~' to stop\n",
      "================================================================\n",
      "\tHello, what can I do for you?\n",
      "----------------------------------------------------------------\n",
      "I want to go to airport\n",
      "\ttopic predicted:  uber\n",
      "\tentering the scenario  uber\n",
      "\tIt seems you want to take ride\n",
      "\tIt seems that you want to say something about:  location\n",
      "----------------------------------------------------------------\n",
      "As soon as possible\n",
      "\tIt seems that you want to say something about:  time\n",
      "----------------------------------------------------------------\n",
      "We have 5 persons\n",
      "\tIt seems that you want to say something about:  num\n",
      "----------------------------------------------------------------\n",
      "My father, my mother, me and my sister\n",
      "\tIt seems that you want to say something about:  num\n",
      "----------------------------------------------------------------\n",
      "How long could it tak\n",
      "\tIt seems that you want to say something about:  type\n",
      "----------------------------------------------------------------\n",
      "How long could it takes\n",
      "\tIt seems that you want to say something about:  type\n",
      "----------------------------------------------------------------\n",
      "thanks!\n",
      "\tIt seems that you want to say something about:  location\n",
      "----------------------------------------------------------------\n",
      "~\n",
      "Goodbye, thank you for your attention\n"
     ]
    }
   ],
   "source": [
    "dc.inConversationDetected()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conversation Test\n",
      "Still working on it, not completed\n",
      "Input '~' to stop\n",
      "================================================================\n",
      "\tHello, what can I do for you?\n",
      "----------------------------------------------------------------\n",
      "Book a restaurant for me\n",
      "\ttopic predicted:  restaurant\n",
      "\tentering the scenario  restaurant\n",
      "\tIt seems you want to book a table\n",
      "\tIt seems that you want to say something about:  num\n",
      "----------------------------------------------------------------\n",
      "Two people, my wife and me\n",
      "\tIt seems that you want to say something about:  num\n",
      "----------------------------------------------------------------\n",
      "Just around the city center\n",
      "\tIt seems that you want to say something about:  location\n",
      "----------------------------------------------------------------\n",
      "I want some real good sea food\n",
      "\tIt seems that you want to say something about:  others\n",
      "----------------------------------------------------------------\n",
      "this evening at 20:00\n",
      "\tIt seems that you want to say something about:  time\n",
      "----------------------------------------------------------------\n",
      "~\n",
      "Goodbye, thank you for your attention\n"
     ]
    }
   ],
   "source": [
    "dc.inConversationDetected()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (Tensorflow)",
   "language": "python",
   "name": "tensorflow"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
